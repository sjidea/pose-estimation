{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torchvision.transforms as tf\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/openpose')\n",
    "from model import PoseEstimationWithMobileNet\n",
    "from include import CocoKeypoints\n",
    "import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 5000\n",
    "lr = 1e-3\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PoseEstimationWithMobileNet().to(device)\n",
    "# model = nn.DataParallel(model).to(device)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay= 1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "        transforms.Normalize(),\n",
    "        transforms.RandomApply(transforms.HFlip(), 0.5),\n",
    "        transforms.RescaleRelative(),\n",
    "        transforms.Crop(368),\n",
    "        transforms.CenterPad(368),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.12s)\n",
      "creating index...\n",
      "index created!\n",
      "filter for keypoint annotations ...\n",
      "... done.\n",
      "Images: 56599\n",
      "Shape of original image = torch.Size([3, 368, 368])\n",
      "Shape of original heatmaps = torch.Size([19, 46, 46])\n",
      "Shape of original pafs = torch.Size([38, 46, 46])\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/var/lib/docker/openpose/coco\"\n",
    "ann_train_dir = [os.path.join(data_dir, 'annotations', item) \n",
    "                 for item in ['person_keypoints_train2017.json']]\n",
    "ann_val_dir = os.path.join(data_dir, 'images/train2017')\n",
    "\n",
    "image_transform = None\n",
    "\n",
    "datas = [CocoKeypoints(\n",
    "            root=ann_val_dir,\n",
    "            annFile=item,\n",
    "            preprocess=preprocess,\n",
    "            image_transform=image_transform,\n",
    "            target_transforms=image_transform, \n",
    "        ) for item in ann_train_dir]\n",
    "\n",
    "data = datas[0]\n",
    "(image, heatmaps, pafs) = data[1]\n",
    "\n",
    "\n",
    "# print(f\"Filepath = {filepath}\")\n",
    "print(f\"Shape of original image = {image.shape}\")\n",
    "print(f\"Shape of original heatmaps = {heatmaps.shape}\")\n",
    "print(f\"Shape of original pafs = {pafs.shape}\")\n",
    "\n",
    "# summary(model, image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368, 368, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5400x2592 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (75, 36))\n",
    "\n",
    "im = image.permute(1, 2, 0)\n",
    "im = np.array(im)\n",
    "print(im.shape)\n",
    "\n",
    "\n",
    "# # im = np.squeeze(image)\n",
    "# # im_ = cm.jet(im)\n",
    "# # im_ = Image.fromarray(np.uint8(im))\n",
    "# # im_ = im_.convert('RGB')\n",
    "\n",
    "# # heatmaps = heatmaps.permute(1,2, 0)\n",
    "# heatmaps = np.array(heatmaps)\n",
    "# heatmap = np.squeeze(heatmaps)  # output 1 is heatmaps\n",
    "# heatmap = heatmap.swapaxes(0, 1).swapaxes(1,2)\n",
    "# heatmap = cv2.resize(heatmap, (0, 0), fx=8, fy=8, interpolation=cv2.INTER_CUBIC)\n",
    "# # heatmap = heatmap[:im.shape[0] - pad[2], :im.shape[1] - pad[3], :]\n",
    "# heatmap = cv2.resize(heatmap, (im.shape[1], im.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# max_value = np.max(heatmap)\n",
    "# min_value = np.min(heatmap)\n",
    "# label = ['nose', 'neck', 'Rsho', 'Relb', 'Rwri', 'Lsho', 'Lelb', 'Lwri', 'Rhip', 'Rkne', 'Rank', 'Lhip', 'Lkne', 'Lank', 'Leye', 'Reye', 'Lear', 'Rear', 'pt19'] \n",
    "# for i in range(18):\n",
    "#     vis_img = (heatmap[:, :, i]-min_value)/max_value\n",
    "#     vis_img = Image.fromarray(np.uint8(cm.jet(vis_img) * 255))\n",
    "#     vis_img = vis_img.convert('RGB') \n",
    "#     vis_img = Image.blend(im_, vis_img, 0.8)\n",
    "#     vis_img = np.array(vis_img)\n",
    "#     plt.subplot(3,6,i+1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(vis_img)\n",
    "#     plt.xlabel(label[i], fontsize=24)\n",
    "     \n",
    "# plt.show()\n",
    "# plt.imshow(im_)\n",
    "\n",
    "\n",
    "# ref_im = cv2.imread(filepath)\n",
    "# i = [2, 1, 0]\n",
    "# ref_im = ref_im[:,:,i]\n",
    "# ref_im_ = Image.fromarray(np.uint8(ref_im))\n",
    "# fig = plt.figure(figsize = (25,12))\n",
    "# plt.show\n",
    "# plt.imshow(ref_im_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pafs = np.array(pafs)\n",
    "# paf = np.squeeze(pafs)  # output 1 is heatmaps\n",
    "# paf = cv2.resize(paf, (0, 0), fx=8, fy=8, interpolation=cv2.INTER_CUBIC)\n",
    "# # paf = paf[:im.shape[0] - pad[2], :im.shape[1] - pad[3], :]\n",
    "# paf = paf.swapaxes(0, 1).swapaxes(1,2)\n",
    "# paf = cv2.resize(paf, (im.shape[1], im.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# print(f\"Shape of original image = {im.shape}\")\n",
    "# print(f\"Shape of heatmap = {heatmap.shape}\")\n",
    "# print(f\"Shape of paf = {paf.shape}\")\n",
    "\n",
    "# fig = plt.figure(figsize = (75, 36))\n",
    "# max_value = np.max(paf)\n",
    "# min_value = np.min(paf)\n",
    "# for i in range(19):\n",
    "#     vis_img_x = (paf[:, :, i*2]-min_value)/max_value\n",
    "#     vis_img_y = (paf[:, :, i*2+1]-min_value)/max_value    \n",
    "#     plt.subplot(3,7,i+1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(im, interpolation='nearest')\n",
    "#     plt.imshow(vis_img_x, cmap=plt.cm.jet, alpha=0.3)\n",
    "#     plt.imshow(vis_img_y, cmap=plt.cm.jet, alpha=0.3)\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.ConcatDataset(datas)\n",
    "\n",
    "n_train = 20000\n",
    "n_val = 5000\n",
    "train_data, val_data, _ = torch.utils.data.random_split(dataset, \n",
    "                                                (n_train, n_val, len(dataset)-n_train-n_val))\n",
    " \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_data, \n",
    "            batch_size = batch, \n",
    "            shuffle = True,\n",
    "            num_workers = 0,\n",
    "            drop_last = True,\n",
    "            )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "            val_data, \n",
    "            batch_size = batch,\n",
    "            shuffle = True,\n",
    "            num_workers = 0,\n",
    "            drop_last = True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(os.getcwd())\n",
    "log_dir = '/openpose/runs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "summary = SummaryWriter('/openpose/runs')\n",
    "# !kill 643\n",
    "%tensorboard --logdir /openpose/runs --port 6004 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti = time.time()\n",
    "save_dir = \"./best_model\"\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    best_val_loss = np.inf\n",
    "    train_tot_loss = 0\n",
    "    val_tot_loss = 0\n",
    "    \n",
    "    ######### TRAIN ########\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for i, (images, heatmaps, pafs) in enumerate(train_loader):\n",
    "        i = i+1\n",
    "        images = images.to(device)\n",
    "        heatmaps = heatmaps.to(device)\n",
    "        pafs = pafs.to(device)\n",
    "    \n",
    "        stages_output = model(images)\n",
    "        paf = stages_output[-1].to(device=device, dtype=torch.long)\n",
    "        heatmap = stages_output[-2].to(device=device, dtype=torch.long)\n",
    "    \n",
    "        train_loss = criterion(heatmaps, heatmap) + criterion(pafs, paf)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.requires_grad = True\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_tot_loss += train_loss.item()\n",
    "\n",
    "        if i==1:\n",
    "            break\n",
    "\n",
    "#     print('out??')\n",
    "    ######## VALIDATION ########\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (images, heatmaps, pafs) in enumerate(val_loader):\n",
    "            i = i+1\n",
    "            images = images.to(device)\n",
    "            heatmaps = heatmaps.to(device)\n",
    "            pafs = pafs.to(device)\n",
    "            \n",
    "            stages_output = model(images)\n",
    "            paf = stages_output[-1].to(device=device, dtype=torch.long)\n",
    "            heatmap = stages_output[-2].to(device=device, dtype=torch.long)\n",
    "            \n",
    "            val_loss = criterion(heatmaps, heatmap) + criterion(pafs, paf)\n",
    "            \n",
    "            val_tot_loss += val_loss.item()\n",
    "            \n",
    "            summary.add_scalar('train_loss', train_loss, epoch)\n",
    "            summary.add_scalar('val_loss', val_loss, epoch)\n",
    "\n",
    "            if i==2:\n",
    "                break\n",
    "\n",
    "                \n",
    "            \n",
    "    scheduler.step(val_loss)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # print status every 10 epoch\n",
    "#     if epoch % 10 == 0 or epoch == n_epoch-1:\n",
    "#         ti_ = time.time()\n",
    "    \n",
    "    if val_loss<best_val_loss:\n",
    "        torch.save(model.state_dict(), save_dir)\n",
    "    \n",
    "# summary.close()\n",
    "# input output 완성하기\n",
    "# initialize\n",
    "# git\n",
    "# gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
